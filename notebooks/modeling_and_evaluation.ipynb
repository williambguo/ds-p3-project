{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I train my classifiers and evaluate their efficacy. Then I attempt to improve my models with hyperparameter tuning. The classifiers I will be building are:\n",
    "- Logistic Regression\n",
    "- Decision Tree\n",
    "- Random Forest\n",
    "- Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cleaned data\n",
    "train = pd.read_csv('../data/final/train_cleaned.csv')\n",
    "test_features = pd.read_csv('../data/final/test_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = train.drop('target', axis=1)\n",
    "train_target = train['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping Unneeded Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unneeded and redundant columns to drop\n",
    "columns_to_drop = ['id', 'wpt_name', 'num_private', 'subvillage', 'ward', 'recorded_by', 'scheme_name', \n",
    "                    'scheme_management', 'water_quality', 'waterpoint_type_group', 'quantity_group', 'region_code', \n",
    "                    'extraction_type', 'extraction_type_group', 'payment', 'source_class', 'source_type',\n",
    "                    'funder', 'installer', 'longitude', 'latitude', 'date_recorded', 'construction_year',\n",
    "                    'district_code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = train_features.drop(columns=columns_to_drop)\n",
    "test_features = test_features.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((59400, 18), (14850, 18))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.shape, test_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding Target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0: <code>functional</code>: pump is functional\n",
    "\n",
    "1: <code>non-functional</code>: pump is not functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    32259\n",
       "1    27141\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target = train_target.map({'functional': 0, 'non functional': 1})\n",
    "train_target.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical features need to be encoded before being fed into classification algorithms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages for categorical encoding\n",
    "import time\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from category_encoders import OrdinalEncoder\n",
    "from category_encoders import CountEncoder\n",
    "from category_encoders import HashingEncoder\n",
    "from category_encoders import BackwardDifferenceEncoder\n",
    "from category_encoders import HelmertEncoder\n",
    "from category_encoders import CatBoostEncoder\n",
    "from category_encoders import GLMMEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following encoding methods will be used:\n",
    "- <code>OrdinalEncoder()</code>: converts categorical variables to ordered numerical values\n",
    "- <code>CountEncoder()</code>: replaces each categorical variable with its frequency count in the column\n",
    "- <code>HashingEncoder()</code>: hashes categorical variables to unique one-hot vectors with configurable dimensionality\n",
    "- <code>BackwardDifferenceEncoder()</code>: the mean of the $k_{th}$ categorical variable is compared with the mean of the $k_{th}-1$\n",
    "- <code>Helmert Encoding()</code>: the mean of the $k_{th}$ categorical variable is compared with the means of all previous categorical variables\n",
    "- <code>CatBoostEncoding()</code>: similar to <code>TargetEncoder</code> which converts each categorical value with the mean appearance of its target, but also involves an ordering principle in order to overcome the \"Target Leakage\" problem\n",
    "- <code>GLMMEncoder()</code>: Generalized Linear Mixed Model - A supervised encoder similar to <code>TargetEncoder</code>, but solid statistical theory is also included. The advantage over other supervised encoders is that no parameter tuning is needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding methods\n",
    "random_state = 42\n",
    "\n",
    "encoding_methods = {\n",
    "    'ordinal': OrdinalEncoder(),\n",
    "    'count': CountEncoder(),\n",
    "    'hashing': HashingEncoder(n_components=32, drop_invariant=True),\n",
    "    'backward_difference': BackwardDifferenceEncoder(),\n",
    "    'Helmert': HelmertEncoder(),\n",
    "    'CatBoost': CatBoostEncoder(random_state=random_state),\n",
    "    'GLMM': GLMMEncoder(random_state=random_state)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifiers\n",
    "classifiers = {\n",
    "    'Logistic Regression': LogisticRegression(n_jobs=-1, random_state=random_state),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=random_state),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'Random Forest': RandomForestClassifier(random_state=random_state, n_jobs=-1),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_features, train_target, test_size=0.2, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- Encoding data using ordinal Encoder -----Done in 0.14s\n",
      "Training Logistic Regression ClassifierDone in 0.24s\n",
      "Training Decision Tree ClassifierDone in 0.2s\n",
      "Training Naive Bayes ClassifierDone in 0.01s\n",
      "Training Random Forest ClassifierDone in 1.22s\n",
      "\n",
      "----- Encoding data using count Encoder -----Done in 0.29s\n",
      "Training Logistic Regression ClassifierDone in 0.34s\n",
      "Training Decision Tree ClassifierDone in 0.19s\n",
      "Training Naive Bayes ClassifierDone in 0.01s\n",
      "Training Random Forest ClassifierDone in 0.77s\n",
      "\n",
      "----- Encoding data using hashing Encoder -----Done in 0.53s\n",
      "Training Logistic Regression Classifier"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/learn-env/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done in 1.18s\n",
      "Training Decision Tree ClassifierDone in 0.24s\n",
      "Training Naive Bayes ClassifierDone in 0.02s\n",
      "Training Random Forest ClassifierDone in 0.74s\n",
      "\n",
      "----- Encoding data using backward_difference Encoder -----"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/learn-env/lib/python3.12/site-packages/category_encoders/base_contrast_encoder.py:126: FutureWarning: Intercept column might not be added anymore in future releases (c.f. issue #370)\n",
      "  warnings.warn(\"Intercept column might not be added anymore in future releases (c.f. issue #370)\",\n",
      "/opt/anaconda3/envs/learn-env/lib/python3.12/site-packages/category_encoders/base_contrast_encoder.py:126: FutureWarning: Intercept column might not be added anymore in future releases (c.f. issue #370)\n",
      "  warnings.warn(\"Intercept column might not be added anymore in future releases (c.f. issue #370)\",\n",
      "/opt/anaconda3/envs/learn-env/lib/python3.12/site-packages/category_encoders/base_contrast_encoder.py:126: FutureWarning: Intercept column might not be added anymore in future releases (c.f. issue #370)\n",
      "  warnings.warn(\"Intercept column might not be added anymore in future releases (c.f. issue #370)\",\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done in 0.87s\n",
      "Training Logistic Regression ClassifierDone in 0.84s\n",
      "Training Decision Tree ClassifierDone in 0.72s\n",
      "Training Naive Bayes ClassifierDone in 0.1s\n",
      "Training Random Forest ClassifierDone in 1.42s\n",
      "\n",
      "----- Encoding data using Helmert Encoder -----"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/learn-env/lib/python3.12/site-packages/category_encoders/base_contrast_encoder.py:126: FutureWarning: Intercept column might not be added anymore in future releases (c.f. issue #370)\n",
      "  warnings.warn(\"Intercept column might not be added anymore in future releases (c.f. issue #370)\",\n",
      "/opt/anaconda3/envs/learn-env/lib/python3.12/site-packages/category_encoders/base_contrast_encoder.py:126: FutureWarning: Intercept column might not be added anymore in future releases (c.f. issue #370)\n",
      "  warnings.warn(\"Intercept column might not be added anymore in future releases (c.f. issue #370)\",\n",
      "/opt/anaconda3/envs/learn-env/lib/python3.12/site-packages/category_encoders/base_contrast_encoder.py:126: FutureWarning: Intercept column might not be added anymore in future releases (c.f. issue #370)\n",
      "  warnings.warn(\"Intercept column might not be added anymore in future releases (c.f. issue #370)\",\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done in 0.79s\n",
      "Training Logistic Regression Classifier"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/learn-env/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done in 1.27s\n",
      "Training Decision Tree ClassifierDone in 0.97s\n",
      "Training Naive Bayes ClassifierDone in 0.11s\n",
      "Training Random Forest ClassifierDone in 1.53s\n",
      "\n",
      "----- Encoding data using CatBoost Encoder -----Done in 0.18s\n",
      "Training Logistic Regression Classifier"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/learn-env/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done in 0.68s\n",
      "Training Decision Tree ClassifierDone in 0.92s\n",
      "Training Naive Bayes ClassifierDone in 0.01s\n",
      "Training Random Forest ClassifierDone in 2.4s\n",
      "\n",
      "----- Encoding data using GLMM Encoder -----Done in 20.95s\n",
      "Training Logistic Regression ClassifierDone in 0.26s\n",
      "Training Decision Tree ClassifierDone in 0.2s\n",
      "Training Naive Bayes ClassifierDone in 0.01s\n",
      "Training Random Forest ClassifierDone in 0.7s\n",
      "\n",
      "Classifier: Logistic Regression\tBest Score: 0.7383838383838384 on Test Data\t Using GLMM Encoder\n",
      "\n",
      "Classifier: Decision Tree\tBest Score: 0.7792087542087542 on Test Data\t Using Helmert Encoder\n",
      "\n",
      "Classifier: Naive Bayes\tBest Score: 0.7301346801346801 on Test Data\t Using CatBoost Encoder\n",
      "\n",
      "Classifier: Random Forest\tBest Score: 0.8118686868686869 on Test Data\t Using GLMM Encoder\n"
     ]
    }
   ],
   "source": [
    "classifier_best_results = {classifier_name: (0, 'encoder') for classifier_name in classifiers.keys()}\n",
    "\n",
    "\n",
    "for encoding_method, encoder in encoding_methods.items():\n",
    "    print(f'\\n----- Encoding data using {encoding_method} Encoder -----', end='')\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    encoded_X_train = encoder.fit_transform(X_train, y_train)\n",
    "    encoded_X_test = encoder.transform(X_test)\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f'Done in {round(end_time-start_time, 2)}s')\n",
    "\n",
    "    for classifier_name, clf_algorithm in classifiers.items():\n",
    "        print(f'Training {classifier_name} Classifier', end='')\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        clf_algorithm.fit(encoded_X_train, y_train)\n",
    "        y_pred = clf_algorithm.predict(encoded_X_test)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "        end_time = time.time()\n",
    "        print(f'Done in {round(end_time-start_time, 2)}s')\n",
    "\n",
    "        previous_acc, _ = classifier_best_results[classifier_name]\n",
    "        if previous_acc < acc:\n",
    "            classifier_best_results[classifier_name] = (acc, encoding_method)\n",
    "\n",
    "\n",
    "for classifier_name, (score, encoding_method) in classifier_best_results.items():\n",
    "    print(f'\\nClassifier: {classifier_name}\\tBest Score: {score} on Test Data\\t Using {encoding_method} Encoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----Classifier: Logistic Regression\tEncoding: GLMM-----\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.87      0.78      6457\n",
      "           1       0.79      0.59      0.67      5423\n",
      "\n",
      "    accuracy                           0.74     11880\n",
      "   macro avg       0.75      0.73      0.73     11880\n",
      "weighted avg       0.75      0.74      0.73     11880\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/learn-env/lib/python3.12/site-packages/category_encoders/base_contrast_encoder.py:126: FutureWarning: Intercept column might not be added anymore in future releases (c.f. issue #370)\n",
      "  warnings.warn(\"Intercept column might not be added anymore in future releases (c.f. issue #370)\",\n",
      "/opt/anaconda3/envs/learn-env/lib/python3.12/site-packages/category_encoders/base_contrast_encoder.py:126: FutureWarning: Intercept column might not be added anymore in future releases (c.f. issue #370)\n",
      "  warnings.warn(\"Intercept column might not be added anymore in future releases (c.f. issue #370)\",\n",
      "/opt/anaconda3/envs/learn-env/lib/python3.12/site-packages/category_encoders/base_contrast_encoder.py:126: FutureWarning: Intercept column might not be added anymore in future releases (c.f. issue #370)\n",
      "  warnings.warn(\"Intercept column might not be added anymore in future releases (c.f. issue #370)\",\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----Classifier: Decision Tree\tEncoding: Helmert-----\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.81      0.80      6457\n",
      "           1       0.76      0.75      0.76      5423\n",
      "\n",
      "    accuracy                           0.78     11880\n",
      "   macro avg       0.78      0.78      0.78     11880\n",
      "weighted avg       0.78      0.78      0.78     11880\n",
      "\n",
      "\n",
      "-----Classifier: Naive Bayes\tEncoding: CatBoost-----\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.85      0.77      6457\n",
      "           1       0.77      0.59      0.67      5423\n",
      "\n",
      "    accuracy                           0.73     11880\n",
      "   macro avg       0.74      0.72      0.72     11880\n",
      "weighted avg       0.74      0.73      0.72     11880\n",
      "\n",
      "\n",
      "-----Classifier: Random Forest\tEncoding: GLMM-----\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.85      0.83      6457\n",
      "           1       0.81      0.77      0.79      5423\n",
      "\n",
      "    accuracy                           0.81     11880\n",
      "   macro avg       0.81      0.81      0.81     11880\n",
      "weighted avg       0.81      0.81      0.81     11880\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "pipelines = []\n",
    "\n",
    "for classifier_name, metric in classifier_best_results.items():\n",
    "    score, encoding_method = metric\n",
    "    encoder = encoding_methods[encoding_method]\n",
    "    encoded_X_train = encoder.fit_transform(X_train, y_train)\n",
    "    encoded_X_test = encoder.transform(X_test)\n",
    "\n",
    "    clf = classifiers[classifier_name]\n",
    "    clf.fit(encoded_X_train, y_train)\n",
    "    y_pred = clf.predict(encoded_X_test)\n",
    "\n",
    "    scores.append(score)\n",
    "    pipelines.append(classifier_name + ' - ' + encoding_method)\n",
    "\n",
    "    print(f'\\n-----Classifier: {classifier_name}\\tEncoding: {encoding_method}-----')\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
