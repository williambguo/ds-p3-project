{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains data cleaning and feature engineering for the test data. It's essentially a carbon copy of training_data_preprocessing.ipynb using the same methods of preprocessing my test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import normalize\n",
    "from skrub import SimilarityEncoder\n",
    "from fancyimpute import IterativeImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the custom functions\n",
    "import sys\n",
    "import os \n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set max display options so that I can see everything I need to see\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "pd.set_option(\"display.max_rows\", 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/processed/test_set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14850 entries, 0 to 14849\n",
      "Data columns (total 40 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   id                     14850 non-null  int64  \n",
      " 1   amount_tsh             4440 non-null   float64\n",
      " 2   date_recorded          14850 non-null  object \n",
      " 3   funder                 13980 non-null  object \n",
      " 4   gps_height             9639 non-null   float64\n",
      " 5   installer              13973 non-null  object \n",
      " 6   longitude              14393 non-null  float64\n",
      " 7   latitude               14393 non-null  float64\n",
      " 8   wpt_name               14850 non-null  object \n",
      " 9   num_private            14850 non-null  int64  \n",
      " 10  basin                  14850 non-null  object \n",
      " 11  subvillage             14751 non-null  object \n",
      " 12  region                 14850 non-null  object \n",
      " 13  region_code            14850 non-null  int64  \n",
      " 14  district_code          14850 non-null  int64  \n",
      " 15  lga                    14850 non-null  object \n",
      " 16  ward                   14850 non-null  object \n",
      " 17  population             9397 non-null   float64\n",
      " 18  public_meeting         14029 non-null  object \n",
      " 19  recorded_by            14850 non-null  object \n",
      " 20  scheme_management      13881 non-null  object \n",
      " 21  scheme_name            7608 non-null   object \n",
      " 22  permit                 14113 non-null  object \n",
      " 23  construction_year      9590 non-null   float64\n",
      " 24  extraction_type        14850 non-null  object \n",
      " 25  extraction_type_group  14850 non-null  object \n",
      " 26  extraction_type_class  14850 non-null  object \n",
      " 27  management             14850 non-null  object \n",
      " 28  management_group       14850 non-null  object \n",
      " 29  payment                14850 non-null  object \n",
      " 30  payment_type           14850 non-null  object \n",
      " 31  water_quality          14850 non-null  object \n",
      " 32  quality_group          14850 non-null  object \n",
      " 33  quantity               14850 non-null  object \n",
      " 34  quantity_group         14850 non-null  object \n",
      " 35  source                 14850 non-null  object \n",
      " 36  source_type            14850 non-null  object \n",
      " 37  source_class           14850 non-null  object \n",
      " 38  waterpoint_type        14850 non-null  object \n",
      " 39  waterpoint_type_group  14850 non-null  object \n",
      "dtypes: float64(6), int64(4), object(30)\n",
      "memory usage: 4.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No exploration or overview of missing data here, just prepping the data for testing my classification models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy 1: Naive approach - Dropping all records with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the columns that are not needed based on my EDA\n",
    "df_test = df.drop(columns=['id', 'wpt_name', 'num_private', 'subvillage', 'ward', 'recorded_by', 'scheme_name', \n",
    "                            'scheme_management', 'water_quality', 'waterpoint_type_group', 'quantity_group', 'region_code', \n",
    "                            'extraction_type', 'extraction_type_group', 'payment', 'source_class', 'source_type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 0's that are not imputed remain as 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of df_train to test dropping all nulls\n",
    "strat1 = df_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# amount_tsh has such as a large number of missing values that it is not \n",
    "# worth keeping for this approach as too many rows would be dropped along with\n",
    "# its missing values\n",
    "strat1 = strat1.drop(columns=['amount_tsh'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all null values\n",
    "strat1 = strat1.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 8069 entries, 0 to 14849\n",
      "Data columns (total 22 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   date_recorded          8069 non-null   object \n",
      " 1   funder                 8069 non-null   object \n",
      " 2   gps_height             8069 non-null   float64\n",
      " 3   installer              8069 non-null   object \n",
      " 4   longitude              8069 non-null   float64\n",
      " 5   latitude               8069 non-null   float64\n",
      " 6   basin                  8069 non-null   object \n",
      " 7   region                 8069 non-null   object \n",
      " 8   district_code          8069 non-null   int64  \n",
      " 9   lga                    8069 non-null   object \n",
      " 10  population             8069 non-null   float64\n",
      " 11  public_meeting         8069 non-null   object \n",
      " 12  permit                 8069 non-null   object \n",
      " 13  construction_year      8069 non-null   float64\n",
      " 14  extraction_type_class  8069 non-null   object \n",
      " 15  management             8069 non-null   object \n",
      " 16  management_group       8069 non-null   object \n",
      " 17  payment_type           8069 non-null   object \n",
      " 18  quality_group          8069 non-null   object \n",
      " 19  quantity               8069 non-null   object \n",
      " 20  source                 8069 non-null   object \n",
      " 21  waterpoint_type        8069 non-null   object \n",
      "dtypes: float64(5), int64(1), object(16)\n",
      "memory usage: 1.4+ MB\n"
     ]
    }
   ],
   "source": [
    "strat1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to csv\n",
    "strat1.to_csv('../data/processed/test_all_nulls_dropped.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy 2: Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only the nearby samples imputation will be done here. See training_data_preprocessing.ipynb for more explanations on everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_k_nearest_by_coordinates(\n",
    "        samples_df: pd.DataFrame,\n",
    "        k_nearest: int,\n",
    "        longitude: float,\n",
    "        latitude: float\n",
    ") -> pd.DataFrame:\n",
    "    geo_coordinates = samples_df[['longitude', 'latitude']]\n",
    "    target_coord = np.float32([longitude, latitude])\n",
    "    distances = np.sqrt(np.sum(np.power(target_coord - geo_coordinates, 2), axis=1))\n",
    "    min_distance_indices = np.argpartition(distances, k_nearest)[1: k_nearest+1]\n",
    "    return samples_df.iloc[min_distance_indices]\n",
    "\n",
    "\n",
    "def impute_zeros_by_nearby_samples(\n",
    "        samples_df: pd.DataFrame,\n",
    "        location_column: str,\n",
    "        target_column: str,\n",
    "        std_threshold: float or None\n",
    ") -> int:\n",
    "    num_imputed = 0\n",
    "\n",
    "    for area in samples_df[location_column].unique():\n",
    "        row_ids = samples_df[location_column] == area\n",
    "        target_values = samples_df.loc[row_ids, target_column]\n",
    "\n",
    "        if target_values.shape[0] > 1:\n",
    "            non_zero_ids = target_values > 0\n",
    "\n",
    "            if non_zero_ids.sum() > 0:\n",
    "                non_zero_values = target_values[non_zero_ids]\n",
    "\n",
    "                if std_threshold is not None and np.std(non_zero_values) > std_threshold:\n",
    "                    continue\n",
    "\n",
    "                zero_ids = np.invert(non_zero_ids)\n",
    "                target_values[zero_ids] = non_zero_values.mean()\n",
    "                samples_df.loc[row_ids, target_column] = target_values\n",
    "                num_imputed += zero_ids.sum()\n",
    "    return num_imputed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### amount_tsh, population, gps_height"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For these imputation methods to work I have to convert the NaN values in my numerical categories back to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'np.nan' values in 'construction_year' with 0\n",
    "df['construction_year'] = df['construction_year'].replace(np.nan, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace np.nan values in 'amount_tsh' with 0\n",
    "df['amount_tsh'] = df['amount_tsh'].replace(np.nan, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace np.nan values in 'gps_height' with 0\n",
    "df['gps_height'] = df['gps_height'].replace(np.nan, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace np.nan values in 'population' with 0\n",
    "df['population'] = df['population'].replace(np.nan, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 0's that are not imputed remain as 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputed 751/10410 missing \"amount_tsh\" values\n"
     ]
    }
   ],
   "source": [
    "amount_tsh_std_threshold = 50\n",
    "\n",
    "num_amount_tsh_missing = (df[\"amount_tsh\"] == 0).sum()\n",
    "num_imputed = impute_zeros_by_nearby_samples(\n",
    "    samples_df=df,\n",
    "    location_column='subvillage',\n",
    "    target_column='amount_tsh',\n",
    "    std_threshold=amount_tsh_std_threshold\n",
    ")\n",
    "print(f'Imputed {num_imputed}/{num_amount_tsh_missing} missing \"amount_tsh\" values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputed 273/5453 missing \"population\" values\n"
     ]
    }
   ],
   "source": [
    "population_std_threshold = 50\n",
    "\n",
    "num_amount_tsh_missing = (df[\"population\"] == 0).sum()\n",
    "num_imputed = impute_zeros_by_nearby_samples(\n",
    "    samples_df=df,\n",
    "    location_column='subvillage',\n",
    "    target_column='population',\n",
    "    std_threshold=population_std_threshold\n",
    ")\n",
    "print(f'Imputed {num_imputed}/{num_amount_tsh_missing} missing \"population\" values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputed 1070/5211 missing \"gps_height\"\n",
      "Imputed 2639/4277 missing \"gps_height\"\n",
      "Imputed 1862/1856 missing \"gps_height\"\n",
      "Imputed 25/25 missing \"gps_height\"\n"
     ]
    }
   ],
   "source": [
    "location_columns = ['subvillage', 'ward', 'lga', 'district_code']\n",
    "\n",
    "for location_column in location_columns:\n",
    "    num_gps_height_missing = (df[\"gps_height\"] == 0).sum()\n",
    "    num_imputed = impute_zeros_by_nearby_samples(\n",
    "        samples_df=df,\n",
    "        location_column=location_column,\n",
    "        target_column='gps_height',\n",
    "        std_threshold=None\n",
    "    )\n",
    "    print(f'Imputed {num_imputed}/{num_gps_height_missing} missing \"gps_height\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gps_height == 0: 0 After K-NN method\n"
     ]
    }
   ],
   "source": [
    "k_neighbors = 25\n",
    "gps_height_zero_ids = df['gps_height'] == 0\n",
    "gps_zero_samples = df[gps_height_zero_ids]\n",
    "gps_heights = []\n",
    "\n",
    "for _, sample in gps_zero_samples.iterrows():\n",
    "    longitude = df['longitude']\n",
    "    latitude = df['latitude']\n",
    "    nearest_samples = find_k_nearest_by_coordinates(\n",
    "        samples_df=df,\n",
    "        k_nearest=k_neighbors,\n",
    "        longitude=longitude,\n",
    "        latitude=latitude\n",
    "    )\n",
    "    non_zero_gps_height_values = nearest_samples.loc[nearest_samples['gps_height'] != 0, 'gps_height']\n",
    "    gps_heights.append(non_zero_gps_height_values.mean())\n",
    "\n",
    "df.loc[df['gps_height'] == 0, 'gps_height'] = gps_heights\n",
    "print(f'gps_height == 0: {(df[\"gps_height\"] == 0).sum()} After K-NN method')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering on the Remaining Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before modeling, the rest of the features could also use a bit of modification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **permit and public_meeting**\n",
    "\n",
    "Encoding these two boolean categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding the booleans in these two categories to 0 (False)m 1(True), and -1 (NaN) \n",
    "df['public_meeting'] = df['public_meeting'].fillna('Unknown')\n",
    "df['public_meeting'] = df['public_meeting'].replace({'False': 0, 'True': 1, 'Unknown': -1})\n",
    "\n",
    "df['permit'] = df['permit'].fillna('Unknown')\n",
    "df['permit'] = df['permit'].replace({'False': 0, 'True': 1, 'Unknown': -1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Creating a pump_age feature**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features 'construction_year' and 'date_recorded' can be used to create a feature that tells us the duration a pump has been in operation. However, 'construction_year' has a lot of missing values. In such cases I'll set pump_age to -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new feature that represents the age of the pump\n",
    "df['construction_year'] = df['construction_year'].replace(0, 10000) # Replacing 0 with 10000 to capture invalid pump ages\n",
    "df['pump_age'] = pd.DatetimeIndex(df['date_recorded']).year - df['construction_year']\n",
    "\n",
    "invalid_pump_age = df['pump_age'] < 0\n",
    "df.loc[invalid_pump_age, 'pump_age'] = -1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Creating a time recorded feature with lower dimensionality 'season'**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insteaf of date recorded or even month recorded I'll condense it down to seasons I discovered while doing EDA - essentially a split between rainy and dry seasons.\n",
    "\n",
    "- ShortDry: January-February\n",
    "- LongRainy: March-May\n",
    "- LongDry: June-October\n",
    "- ShortRainy: November-December"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary matching months to their corresponding seasons\n",
    "# 0: ShortDry, 1: LongRainy, 2: LongDry, 3: ShortRainy\n",
    "seasons = {\n",
    "    1: 0, 2: 0,\n",
    "    3: 1, 4: 1, 5: 1,\n",
    "    6: 2, 7: 2, 8: 2, 9: 2, 10: 2,\n",
    "    11: 3, 12: 3,\n",
    "}\n",
    "\n",
    "# Creating the 'season' column\n",
    "df['season'] = pd.DataFrame({'Month': pd.DatetimeIndex(df['date_recorded']).month})\n",
    "df['season'] = df['season'].apply(lambda month: seasons[month])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14850 entries, 0 to 14849\n",
      "Data columns (total 42 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   id                     14850 non-null  int64  \n",
      " 1   amount_tsh             14850 non-null  float64\n",
      " 2   date_recorded          14850 non-null  object \n",
      " 3   funder                 13980 non-null  object \n",
      " 4   gps_height             14850 non-null  float64\n",
      " 5   installer              13973 non-null  object \n",
      " 6   longitude              14393 non-null  float64\n",
      " 7   latitude               14393 non-null  float64\n",
      " 8   wpt_name               14850 non-null  object \n",
      " 9   num_private            14850 non-null  int64  \n",
      " 10  basin                  14850 non-null  object \n",
      " 11  subvillage             14751 non-null  object \n",
      " 12  region                 14850 non-null  object \n",
      " 13  region_code            14850 non-null  int64  \n",
      " 14  district_code          14850 non-null  int64  \n",
      " 15  lga                    14850 non-null  object \n",
      " 16  ward                   14850 non-null  object \n",
      " 17  population             14850 non-null  float64\n",
      " 18  public_meeting         14850 non-null  object \n",
      " 19  recorded_by            14850 non-null  object \n",
      " 20  scheme_management      13881 non-null  object \n",
      " 21  scheme_name            7608 non-null   object \n",
      " 22  permit                 14850 non-null  object \n",
      " 23  construction_year      14850 non-null  float64\n",
      " 24  extraction_type        14850 non-null  object \n",
      " 25  extraction_type_group  14850 non-null  object \n",
      " 26  extraction_type_class  14850 non-null  object \n",
      " 27  management             14850 non-null  object \n",
      " 28  management_group       14850 non-null  object \n",
      " 29  payment                14850 non-null  object \n",
      " 30  payment_type           14850 non-null  object \n",
      " 31  water_quality          14850 non-null  object \n",
      " 32  quality_group          14850 non-null  object \n",
      " 33  quantity               14850 non-null  object \n",
      " 34  quantity_group         14850 non-null  object \n",
      " 35  source                 14850 non-null  object \n",
      " 36  source_type            14850 non-null  object \n",
      " 37  source_class           14850 non-null  object \n",
      " 38  waterpoint_type        14850 non-null  object \n",
      " 39  waterpoint_type_group  14850 non-null  object \n",
      " 40  pump_age               14850 non-null  float64\n",
      " 41  season                 14850 non-null  int64  \n",
      "dtypes: float64(7), int64(5), object(30)\n",
      "memory usage: 4.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to csv file\n",
    "df.to_csv('../data/final/test_cleaned.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
